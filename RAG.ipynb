{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3300f-5be4-4374-aa2a-d891ca550a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece76991-a503-40f4-9756-c911f3ee0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255e9ea-c830-487d-adc7-6c115edf4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e96edb-8468-4ec6-b961-483e1a6cb765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a file from your computer to upload.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76caea3a-509e-41e9-9720-597918b9b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "#name your pdf document same as the name in the file uploaded.\n",
    "pdf_document = \"a brief history of time.pdf\"\n",
    "doc = fitz.open(pdf_document)\n",
    "\n",
    "#this creates text chunks out of PDF.\n",
    "text = \"\"\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc.load_page(page_num)\n",
    "    text += page.get_text()\n",
    "\n",
    "def split_text(text, max_length=500):\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(' '.join(current_chunk)) + len(sentence) <= max_length:\n",
    "            current_chunk.append(sentence)\n",
    "        else:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "text_chunks = split_text_with_overlap(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839bddc1-9979-4763-aabf-6b5cc92c7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will do embeddings\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603b9ee-d3b8-41d8-b300-7834bf8d1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b37d7b-f231-4dab-b91c-58de329ea20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb886e05-4ff7-4a04-b4d9-7bec0814bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c3a2b-743d-4363-ba8d-1234880d66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.array(embeddings)\n",
    "\n",
    "index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
    "\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "faiss.write_index(index, \"faiss_index.index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa41ea-e5b3-41c7-9f9b-9730009a2cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, T5Tokenizer\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "flan_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e9a1b-69bd-4702-8265-5abb1617684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will take your queries, answer them until you press \"q\".\n",
    "while(True):\n",
    "\n",
    "  query = input(\"--->>>\")\n",
    "  if query == 'q':\n",
    "    break\n",
    "  query_embedding = model.encode([query])\n",
    "\n",
    "  D, I = index.search(np.array(query_embedding), k=7)\n",
    "\n",
    "  retrieved_chunks = [text_chunks[i] for i in I[0]]\n",
    "\n",
    "  retrieved_context = \" \".join(retrieved_chunks)\n",
    "  # Check what context is retrieved by inserting print(retrieved_chunks) here.\n",
    "  print(retrieved_context)\n",
    "\n",
    "  input_text = f\"Answer the question based on the following context: Question: {query}, Context: {retrieved_context} \"\n",
    "  inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "  outputs = flan_model.generate(inputs[\"input_ids\"], max_length=2000,num_beams=4)\n",
    "  answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "  print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c2a60-d8a5-4cc4-8cb0-c254d61430cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
